{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4daa6134",
      "metadata": {
        "id": "4daa6134"
      },
      "source": [
        "\n",
        "#### Climate Sentiment Classification with BERT\n",
        "\n",
        "In this notebook, we will go through the steps to train a sentiment analysis model using the BERT transformer model. We will:\n",
        "1. Load and prepare the dataset.\n",
        "2. Tokenize the text data.\n",
        "3. Split the data into training and evaluation sets.\n",
        "4. Train the model.\n",
        "5. Test the model on a set of predefined prompts.\n",
        "\n",
        "Let's begin!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install** Huggingface packages for both transformer models and datasets"
      ],
      "metadata": {
        "id": "ak-8lRi47BPY"
      },
      "id": "ak-8lRi47BPY"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers datasets"
      ],
      "metadata": {
        "id": "rJSQI6TfAx55"
      },
      "id": "rJSQI6TfAx55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9c57e218",
      "metadata": {
        "id": "9c57e218"
      },
      "source": [
        "\n",
        "#### Step 1: Define Functions\n",
        "\n",
        "We will define the following functions to organize our code:\n",
        "1. `load_and_prepare_data(file_path)`: Loads and tokenizes the dataset.\n",
        "2. `tokenize_dataset(examples)`: Tokenizes text data.\n",
        "3. `select_subsets(tokenized_datasets, subset_size_ratio)`: Splits the dataset into training and evaluation subsets.\n",
        "4. `initialize_trainer(train_dataset, eval_dataset)`: Initializes the Trainer for BERT.\n",
        "5. `test_model_performance(sentiment_pipeline, prompts)`: Tests the model on given prompts and prints predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ccbf5cd",
      "metadata": {
        "id": "7ccbf5cd"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset and prepare for training\n",
        "def load_and_prepare_data(file_path):\n",
        "    dataset = load_dataset('csv', data_files={'train': file_path})\n",
        "    tokenized_datasets = dataset.map(tokenize_dataset, batched=True)\n",
        "    return tokenized_datasets\n",
        "\n",
        "# Tokenize text data for BERT\n",
        "def tokenize_dataset(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "# Select training and evaluation subsets\n",
        "def select_subsets(tokenized_datasets, subset_size_ratio=0.8):\n",
        "    subset_size = min(1000, len(tokenized_datasets['train']))\n",
        "    train_size = int(subset_size * subset_size_ratio)\n",
        "    small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(train_size))\n",
        "    small_eval_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(train_size, subset_size))\n",
        "    return small_train_dataset, small_eval_dataset\n",
        "\n",
        "# Initialize and return the Trainer\n",
        "def initialize_trainer(train_dataset, eval_dataset):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"/results\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        # only one epoch for demostration purposes\n",
        "        num_train_epochs=1,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "    )\n",
        "    return trainer\n",
        "\n",
        "# Evaluate model performance on test prompts\n",
        "def test_model_performance(sentiment_pipeline, prompts):\n",
        "    for prompt in prompts:\n",
        "        print(f\"Prompt: {prompt}\\nPrediction: {sentiment_pipeline(prompt)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edd3da39",
      "metadata": {
        "id": "edd3da39"
      },
      "source": [
        "\n",
        "#### Step 2: Main Program\n",
        "\n",
        "Here, we initialize the tokenizer and model, load the dataset, and proceed with training the model. We will evaluate the model's performance before and after training on a set of predefined climate-related prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef78225b",
      "metadata": {
        "id": "ef78225b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize tokenizer and model for BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Main flow\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = 'https://jerrycuomo.github.io/Think_Artificial_Intelligence/datasets/climatebert-climate-sentiment.csv'\n",
        "    tokenized_datasets = load_and_prepare_data(dataset_path)\n",
        "    small_train_dataset, small_eval_dataset = select_subsets(tokenized_datasets)\n",
        "\n",
        "    trainer = initialize_trainer(small_train_dataset, small_eval_dataset)\n",
        "    print(\"Before Training:\")\n",
        "    sentiment_pipeline_before = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    test_prompts = [\n",
        "        \"The company has achieved a 20% reduction in water usage over the past year through improved conservation efforts.\",\n",
        "        \"Recent audits revealed non-compliance with environmental regulations in several of our manufacturing facilities.\",\n",
        "        \"Our new product line uses recycled materials, contributing to a circular economy and reducing waste.\",\n",
        "        \"Emissions have increased due to expanded operations.\",\n",
        "        \"The company is currently evaluating the environmental impact of its operations to better align with sustainability goals.\"\n",
        "    ]\n",
        "\n",
        "    test_model_performance(sentiment_pipeline_before, test_prompts)\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"After Training:\")\n",
        "    sentiment_pipeline_after = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "    test_model_performance(sentiment_pipeline_after, test_prompts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c5933e",
      "metadata": {
        "id": "14c5933e"
      },
      "source": [
        "\n",
        "#### Conclusion\n",
        "\n",
        "In this notebook, we walked through the process of training a BERT model for sentiment analysis on climate-related data. We saw how to:\n",
        "1. Load and prepare the dataset.\n",
        "2. Tokenize the data for BERT.\n",
        "3. Train the model using the `Trainer` class.\n",
        "4. Evaluate the model's performance before and after training.\n",
        "\n",
        "Great work!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}